## ãƒ•ã‚¡ã‚¤ãƒ«: app.py

```python
# -*- coding: utf-8 -*-
# Streamlit UI for local RAG tool (ä¿®æ­£ç‰ˆ)
# - Ollama ãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—ã®å …ç‰¢åŒ–
# - retrieval-only ãƒ¢ãƒ¼ãƒ‰ã® UI çµ±åˆ
# - ä¿®æ­£ç®‡æ‰€ã«è¡Œæœ«ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ä»˜ä¸

import streamlit as st
import os
import json
import time
from rag_core import (
    build_index_from_folder,
    list_indexes,
    list_ollama_models,
    query_index,
    request_cancel,
    reset_cancel,
)

st.set_page_config(page_title="RAG Builder", layout="wide")
st.title("ğŸ“ ãƒ­ãƒ¼ã‚«ãƒ« RAG ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆãƒ»æ¤œç´¢ãƒ„ãƒ¼ãƒ«")

# ------------------------------
# é€²æ—ãƒãƒ¼é ˜åŸŸ
# ------------------------------
file_phase = st.empty()
file_bar = st.progress(0)
chunk_phase = st.empty()
chunk_bar = st.progress(0)

# callback wrappers

def file_progress(idx, total, path):
    try:
        pct = idx / total if total else 0
    except Exception:
        pct = 0
    file_phase.write(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {idx}/{total} - {path}")
    try:
        file_bar.progress(pct)
    except Exception:
        file_bar.progress(0)


def chunk_progress(idx, total):
    try:
        pct = idx / total if total else 0
    except Exception:
        pct = 0
    chunk_phase.write(f"ğŸ§© ãƒãƒ¼ãƒ‰å‡¦ç†: {idx}/{total}")
    try:
        chunk_bar.progress(pct)
    except Exception:
        chunk_bar.progress(0)

st.markdown("All processing is **fully local**. Files never leave your machine.")

# Sidebar settings
with st.sidebar:
    st.header("Settings")
    storage_dir = st.text_input("Storage directory", value=os.getenv("STORAGE_DIR", "storage"))

    # HuggingFace Embedding Model Path
    embed_model_path = st.text_input(
        "Embedding model (local HuggingFace folder)",
        value=os.getenv("EMBED_MODEL_PATH", "local_models/all-MiniLM-L6-v2")
    )

    st.markdown("---")
    st.subheader("Ollama")
    models = []
    try:
        models = list_ollama_models()
    except Exception:
        models = []

    if models:
        llm_model = st.selectbox("Ollama ãƒ¢ãƒ‡ãƒ«é¸æŠ", models, index=0)
    else:
        llm_model = st.text_input("Ollama Model (manual)", value=os.getenv("OLLAMA_LLM_MODEL", "llama3.2:3b"))

    st.markdown("### Existing Indexes")
    idxs = list_indexes(storage_dir)
    st.write(idxs)

    if st.button("Refresh List"):
        idxs = list_indexes(storage_dir)
        st.write(idxs)

# ------------------------------
# ã‚¿ãƒ– UI
# ------------------------------
tabs = st.tabs(["ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ", "æ¤œç´¢"])

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚¿ãƒ–
with tabs[0]:
    st.header("ğŸ“˜ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ")
    folder = st.text_input("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ã®ãƒ‘ã‚¹")
    index_name = st.text_input("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å", value="myindex")

    embed_model = st.text_input("HuggingFace åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«", value=embed_model_path)
    chunk_size = st.number_input("chunk_size", 32, 4096, 512)
    chunk_overlap = st.number_input("chunk_overlap", 0, 2048, 100)

    if st.button("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆé–‹å§‹", use_container_width=True):
        reset_cancel()
        file_phase.write("")
        chunk_phase.write("")
        file_bar.progress(0)
        chunk_bar.progress(0)

        if not folder:
            st.error("ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        else:
            st.info("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆä¸­... ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚")
            res = build_index_from_folder(
                folder=folder,
                index_name=index_name,
                embed_model_path=embed_model,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                progress_callback=file_progress,
                chunk_callback=chunk_progress,
            )

            if res.get("status") == "ok":
                st.success("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆå®Œäº†ï¼")
            elif res.get("status") == "cancelled":
                st.warning("ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
            else:
                st.error(f"ã‚¨ãƒ©ãƒ¼: {res.get('message')}")
                st.json(res)

    if st.button("ã‚­ãƒ£ãƒ³ã‚»ãƒ«", use_container_width=True):
        request_cancel()
        st.warning("ã‚­ãƒ£ãƒ³ã‚»ãƒ«è¦æ±‚ã‚’é€ä¿¡ã—ã¾ã—ãŸã€‚")

# æ¤œç´¢ã‚¿ãƒ–
with tabs[1]:
    st.header("ğŸ” ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¤œç´¢")

    # æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è¦§
    indexes = list_indexes(storage_dir)
    index_sel = st.selectbox("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’é¸æŠ", indexes)

    query = st.text_area("æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›")

    # LLM ä½¿ç”¨æœ‰ç„¡
    llm_use = st.checkbox("LLM ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã‚’ç”Ÿæˆã™ã‚‹ (ã‚ªãƒ•ãªã‚‰å˜ç´”æ¤œç´¢)", value=False)

    if llm_use:
        st.write(f"é¸æŠãƒ¢ãƒ‡ãƒ«: {llm_model}")

    top_k = st.number_input("Retriever top_k", 1, 50, 5)

    if st.button("æ¤œç´¢å®Ÿè¡Œ", use_container_width=True):
        if not index_sel:
            st.error("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“")
        elif not query:
            st.error("ã‚¯ã‚¨ãƒªãŒç©ºã§ã™")
        else:
            st.info("æ¤œç´¢ä¸­...")
            res = query_index(
                index_name=index_sel,
                query=query,
                embed_model_path=embed_model,
                llm_model_name=llm_model,
                use_llm=llm_use,
                top_k=top_k,
            )

            if res.get("status") == "ok":
                st.success("æ¤œç´¢å®Œäº†")
                st.json(res.get("response"))
            else:
                st.error(f"ã‚¨ãƒ©ãƒ¼: {res.get('error')}")
